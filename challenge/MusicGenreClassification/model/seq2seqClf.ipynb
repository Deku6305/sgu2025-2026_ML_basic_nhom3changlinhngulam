{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f070b0c7",
   "metadata": {},
   "source": [
    "# Mục tiêu là sử dụng mô hình seq2seq để classification tất cả các nghệ sĩ đã biết (đã label) \n",
    "+ sau khi train xong thì loại bỏ FC cuối để lấy đc embedding vector của các tác giả"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471fdca",
   "metadata": {},
   "source": [
    "## Import thư viện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b3a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from itables import show  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57106f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, loss=148.1392\n",
      "Epoch 2/10, loss=110.9839\n",
      "Epoch 3/10, loss=109.2219\n",
      "Epoch 4/10, loss=108.2468\n",
      "Epoch 5/10, loss=108.1615\n",
      "Epoch 6/10, loss=107.8575\n",
      "Epoch 7/10, loss=107.8486\n",
      "Epoch 8/10, loss=107.7961\n",
      "Epoch 9/10, loss=107.9999\n",
      "Epoch 10/10, loss=107.4204\n",
      "Test Accuracy: 0.8226433430515063\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "# -----------------------------\n",
    "# 1. Load vocab\n",
    "# -----------------------------\n",
    "vocab_df = pd.read_csv(\"../exps/Preproccessed/exp2_vocab.csv\")  # columns: word,id\n",
    "vocab = dict(zip(vocab_df[\"Artist Name\"], vocab_df['Class']))\n",
    "vocab_size = len(vocab) + 1  # +1 for unknown token\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load train/test\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(\"../exps/Preproccessed/exp2_NamesLabeling_Train.csv\")  # columns: text,label\n",
    "test_df  = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Tokenizer for comma-separated text\n",
    "# -----------------------------\n",
    "def text_to_ids(text, vocab):\n",
    "    # Split by comma, strip spaces, lowercase if needed\n",
    "    tokens = [tok.strip() for tok in re.split(r',\\s*', text)]\n",
    "    ids = [vocab.get(tok, 0) for tok in tokens]  # unknown token -> 0\n",
    "    return ids\n",
    "\n",
    "train_df['seq'] = train_df['Artist Name'].apply(lambda x: text_to_ids(x, vocab))\n",
    "test_df['seq']  = test_df['Artist Name'].apply(lambda x: text_to_ids(x, vocab))\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Encode labels to integers\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "train_df['Class'] = le.fit_transform(train_df['Class'])\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Prepare sequences and Class\n",
    "# -----------------------------\n",
    "train_sequences = train_df['seq'].tolist()\n",
    "train_Class    = train_df['Class'].tolist()\n",
    "\n",
    "test_sequences = test_df['seq'].tolist()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_sequences, train_Class, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. PyTorch Dataset\n",
    "# -----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return padded, torch.tensor(labels)\n",
    "\n",
    "train_dataset = TextDataset(train_sequences, train_Class)\n",
    "val_dataset   = TextDataset(X_val, y_val)\n",
    "# test_dataset  = TextDataset(test_sequences, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Define LSTM model\n",
    "# -----------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=1, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self.embedding(x)\n",
    "        out, (h, c) = self.lstm(x)\n",
    "        last_hidden = h[-1]  # shape: (batch, hidden_dim)\n",
    "        if return_embedding:\n",
    "            return last_hidden  # return vector instead of logits\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Training setup\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_classes=11,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Training loop\n",
    "# -----------------------------\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, loss={total_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Evaluation\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += len(y_batch)\n",
    "\n",
    "print(\"Test Accuracy:\", correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf20c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3600, 256])\n"
     ]
    }
   ],
   "source": [
    "test_dataset  = TextDataset(test_sequences, [0]*len(test_sequences))  # dummy labels\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch_emb = model(X_batch, return_embedding=True)  # shape: (batch, hidden_dim)\n",
    "        embeddings.append(batch_emb.cpu())\n",
    "\n",
    "# Concatenate all batches\n",
    "embeddings = torch.cat(embeddings, dim=0)  # shape: (num_samples, hidden_dim)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad73c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test embeddings to test_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emb_df = pd.DataFrame(embeddings.numpy())\n",
    "emb_df.to_csv(\"../exps/Preproccessed/test_embeddings.csv\", index=False)\n",
    "print(\"Saved test embeddings to test_embeddings.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
